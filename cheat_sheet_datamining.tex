\documentclass[a4paper]{article}

\usepackage{pdflscape}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{color}
\usepackage{enumitem}

\usepackage[left=10mm, right=10mm, top=10mm, bottom=10mm]{geometry}

\usepackage{titlesec}

\usepackage[utf8]{inputenc}
\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}
\usepackage{amsmath}


\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\titlespacing\section{0pt}{5pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{5pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{5pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}



\setlength{\columnseprule}{0.5pt}
\def\columnseprulecolor{\color{black}}

\pagenumbering{gobble}

\title{DataMining Cheat Sheet}
\author{Niklas Ullmann}
\date{Summer 2022}


\begin{document}
\begin{landscape}
    \thispagestyle{empty}

    \begin{multicols}{3}
        
    \section{Grundlagen}
        \begin{itemize}[noitemsep,nolistsep]
            \item Qualitative Attribute: 
                \begin{itemize}
                    \item Variieren nach Beschaffenheit
                \end{itemize}
            \item Quantitative Attribute:
                \begin{itemize}
                    \item Variieren nach Wert/Zahlen
                \end{itemize}
        \end{itemize}
        \begin{itemize}[noitemsep,nolistsep]
            \item Diskrete Attribute: 
                \begin{itemize}
                    \item abgestufte Werte
                \end{itemize}
            \item Stetige Attribute:
                \begin{itemize}
                    \item können im Intervall jeden reellen Wert annehmen
                \end{itemize}
        \end{itemize}
            \subsection{Skalenniveaus}
            \begin{itemize}[noitemsep,nolistsep]
                \item Nominal
                \begin{itemize}
                    \item nur Gleichheit oder Andersartigkeit feststellbar (keine Bewertung)
                    \item stets qualitativ
                \end{itemize}
                \item Ordinal
                \begin{itemize}
                    \item natürliche oder festzulegende Rangfolge
                \end{itemize}
                \item Kardinal/Metrisch
                \begin{itemize}
                    \item numerischer Art 
                    \item Ausprägung und Unterschied sind messbar
                    \item verhältnisskaliert (Absoluter Nullpunkt vorhanden; (Doppelt so viel.))
                    \item intervallskaliert (Kein Nullpunkt, nur Differenzen)
                \end{itemize}
            \end{itemize}
    \subsection{Sym. vs asym. Attribute}
        \begin{itemize}[noitemsep,nolistsep]
            \item Das symmetrische binäre Attribut ist ein Attribut, bei dem jeder Wert gleichwertig ist (w/m)
            \item Asymmetrisch ist ein Attribut, bei dem die beiden Ausprägungen nicht gleichwertig sind (Testergebnisse oder Vergleich von Umfragen)
        \end{itemize}

    \subsection{Rauschen Artefakte, Ausreißer}
    \begin{itemize}[noitemsep,nolistsep]
        \item Rauschen (Random Verzerrung der Messung durch Einflussfaktoren)
        \item Artefakte (Unvollständige Messwerte)
        \item Ausreißer (Messwerte, die nicht im Normalbereich liegen)
    \end{itemize}

    \subsection{Datenvorverarbeitung}

    \begin{itemize}[noitemsep,nolistsep]
        \item Aggregation (Zusammenfassung mehrerer Messwerte, Details gehen verloren)
        \item Sampling
        \item Diskretisierung / Binarisierung
        \item Transformation
        \item Dimensionsreduktion
        \item Feature Subset Selection (Konzentration auf wichtige Features)
        \item Feature Creation
    \end{itemize}

    \subsection{Ähnlichkeits- und Distanzmaße}
            \subsubsection{Ähnlichkeit}

            Eigenschaften:
            \begin{itemize}[noitemsep,nolistsep]
                \item $s(x,y) 0 <= s <= 1$
                \item $s(x,y) = 0$, wenn $x = y$
                \item Symmetry: $s(x,y) = s(y,x)$
            \end{itemize}

            \textbf{Simple Matching Coefficient (SMC):}
            \begin{itemize}[noitemsep,nolistsep]
                \item $ SMC = \dfrac{f_{00}+f_{11}}{f_{01}+ f_{10}+ f_{00}+ f_{11}} $
                \item Binäre Daten
                \item gut für \textbf{sym. Attribute}, da Vorhandensein und Abwesenheit gleich gewertet wird
            \end{itemize}

            \textbf{Jaccard Coefficient:}
            \begin{itemize}[noitemsep,nolistsep]
                \item $ J = \dfrac{f_{11}}{f_{01}+ f_{10}+ f_{11}} $
                \item Binäre Daten
                \item gut für \textbf{asym. Attribute}, da Vorhandensein gewertet wird
            \end{itemize}

            \textbf{Extended Jaccard Coefficient (Tanimoto):}
            \begin{itemize}[noitemsep,nolistsep]
                \item $EJ: \dfrac{\langle x, y \rangle}{||x||^2 + ||y||^2 - \langle x,y \rangle}$
                \item Jaccard für alle Daten
            \end{itemize}

            \textbf{Cosine Similarity:}
            \begin{itemize}[noitemsep,nolistsep]
                \item $cos(x,y) =  \dfrac{\langle x, y \rangle}{||x|| * ||y||}$
                \item $ -1 <= cos(x,y) <= 1$
                \item 1 = sehr ähnlich, 0 = Vekrtor im 90° Winkel, -1 = Vektor im 180° Winkel
                \item Umrechnung von zahl zu Winkel im Taschenrechner mit $cos^{-1}$
                \item auch für asym. Attribute da 0-0 Paare rausfallen
            \end{itemize}

            \textbf{Correlation:}
            \begin{itemize}[noitemsep,nolistsep]
                \item $corr(x,y)$ über Taschenrechner
                \item zeigt linearen Zusammenhang
            \end{itemize}


            \subsubsection{Distanz (Minkowski)}

            Eigenschaften:
            \begin{itemize}[noitemsep,nolistsep]
                \item Positivity (d(x,y) >= 0, d(x,y) = 0, wenn x = y)
                \item Symmetry (d(x,y)= d(y,x))
                \item Triangle Inequality (d(x,z) <= d(x,y) + d(y,z))
            \end{itemize}
            
            $$ d(x,y) =  \sqrt[r]{\sum^{n}_{k=1} | x_k - y_k |^r} $$

            \begin{center}
                
                \begin{tabular}{|l|l|l|}
                \hline
                Name      & r   & Anwendung                                                                                            \\ \hline
                Hamming   & 1   & Bin.Vekt. \\ \hline
                CityBlock & 1   &  nur gerade                                                                                                    \\ \hline
                Euclid    & 2   &  schräg                                                                                                    \\ \hline
                Supremum  & $\infty$ &  nur größte Dist.                                                                                                 \\ \hline
                \end{tabular}
            \end{center}

            \subsubsection{Weiteres}

            \textbf{Verhalten für Multiplikation und Addition:}
            \begin{center}
            \begin{tabular}{|l|l|l|l|}
                \hline
                Property                    & Cosine & Correlation & Minkowski \\ \hline
                Invariant to multiplication & Yes    & Yes         & No        \\ \hline
                Invariant to addition   & No     & Yes         & No        \\ \hline
                \end{tabular}
            \end{center}
        
            \textbf{Mutual Information:}
            \begin{itemize}[noitemsep,nolistsep]
                \item Ähnlich wie Correlation, aber für nicht linearen Zusammenhang
                \item $0 =$ kein Zusammenhang, $1 =$ starker Zusammenhang
                \item \textcolor{red}{HIER fehlts}
            \end{itemize}
            
            \textbf{Umrechnung Ähnlichkeit $<->$ Distanz:}\\
            Bspw:
            \begin{itemize}[noitemsep,nolistsep]
                \item $s = \dfrac{1}{d}-1$
                \item $s = ln(x)*-1$
                \item $d = 1 - s$
                \item $d = \sqrt[2]{1-s}$
            \end{itemize}

    \section{Klassifikation}
            \begin{itemize}[noitemsep,nolistsep]
                \item Zuordnung einer abhängigen Variable (y) anhand von unanhängigen Variablen
                \item Model hat beim Training (Induction) gelernt zuzuordnen
                \item Model wendet das gelernte bei der Klassifikation an (Deduction)
            \end{itemize}
        \subsection{Beispiele von Klassifikationsverfahren}
        \begin{itemize}[noitemsep,nolistsep]
            \item Elementare Verfahren (Decision Trees, KNN, Naive Bays, SVM, NN)
            \item Ensemble Verfahren (Random Forests, bagging, Boosting, \dots)
        \end{itemize}
        \subsection{Entscheidungsbäume}
        \begin{itemize}[noitemsep,nolistsep]
            \item Datensatz durchläuft von der Wurzel bis zum Blatt die Knoten und wird anhand der Entscheidungen am Knoten klassifiziert
            \item Hunts Algo entscheidet, wie Splits gesetzt werden (gibt noch mehr)
        \end{itemize}
        \subsubsection{Hunts Algo}
        \begin{itemize}[noitemsep,nolistsep]
            \item Sei $D_t$ die Menge der Trainingsdatensätze, die Knoten t erreichen
            \item Wenn $D_t$ nur Datensätze enthält, die zur selben Klasse ytgehören, dann ist t ein Blatt des Baumes und wird mit ytgekennzeichnet.
            \item Falls $D_t$ Datensätze enthält, die zu mehr als einer Klasse gehören, verwende eine Attribut-Testbedingung, um die Daten in kleinere Untermengen aufzuteilen
        \end{itemize}

        \subsubsection{Split bei Attributen}
        \begin{itemize}[noitemsep,nolistsep]
            \item Binärer Split
            \item Mehrfach Split
        \end{itemize}
        Möglichkeiten der Diskretisierung
        \begin{itemize}[noitemsep,nolistsep]
            \item Einteilung in gleichbelegte Bereiche (Percentile)
            \item Einteilung in gleiche Bereiche (Clustering)
            \item Binäre Entscheidung: (A < v) und (A >= v)
        \end{itemize}
        \textbf{Greedy Ansatz}
        Algorithmus der schrittweise den besten nächsten Schritt mit dem höchsten gewinn wählt.

        \subsection{Maß für Knotenunreinheit}
        \begin{itemize}[noitemsep,nolistsep]
            \item $p_i(t)$ Häufigkeit von klasse i beim Knoten t\\ $c$ Gesamtzahl der Klassen
            \item \textbf{Gini Index}
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ GI = 1 - \sum_{i=0}^{c-1} p_i(t)^2$
                        \item Maximum: $1-\dfrac{1}{c}$
                        \item Minimum: 0 (Best Case)
                    \end{itemize}
            \item \textbf{Entropy}
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ E = - \sum_{i=0}^{c-1} p_i(t) * log_2 p_i(t)$
                        \item Maximum: $log_2 c$
                        \item Minimum: 0 (Best Case)
                    \end{itemize}
            \item \textbf{Klassifikationsfehler}
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ CE = 1 - max[p_i(t)]$
                        \item Maximum: Wenn alle Datensätze auf die Klassen gleich verteilt sind 
                        \item Minimum: 0 (Best Case, wenn alle datensätze zu einer Klasse gehören)
                    \end{itemize}
        \end{itemize}

        \subsection{Nachfolgende Berechnungen}
        \begin{itemize}[noitemsep,nolistsep]
            \item Können mit allen 3 Maßen berechnet werden.
            \item Split
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ split = \sum_{i = 1}^{k} \dfrac{n_i}{n} * Knotenunreinheit$
                        \item $n_i$ = Anzahl der Daten im Kindknoten i
                        \item $n$ = Anzahl der Daten im Elternknoten
                    \end{itemize}
            \item Gain
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ gain = P - M$
                        \item $P$ = Knotenunreinheit des Elternknoten
                        \item $M$ = Split der Kindknoten
                        \item Gain maximieren für einen guten Split bzw. M minimieren!!!
                    \end{itemize}
            \item \textcolor{red}{Problem: Splits mit vielen Kindsknoten mit wenigen aber einen Datensätze werden bevorzugt!}
            \item SplitInfo
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ splitInfo = \sum_{i = 1}^{k} \dfrac{n_i}{n} log_2 \dfrac{n_i}{n}$
                        \item $splitInfo$ = Entropie der Partitionierung
                    \end{itemize}
            \item GainRatio
                    \begin{itemize}[noitemsep,nolistsep]
                        \item $ gainRatio = \dfrac{gain_{split}}{splitInfo}$
                        \item Korrigierter Gain um Entropie -> Bestrafung hoher Anzahl kleiner Partitionen
                        \item Maximum (Best Case)
                    \end{itemize}
        \end{itemize}

        \subsection{Bewertung Bäume, Overfitting etc.}
        
        \subsection{Modell Evaluation}

    \section{Clustering}
    
    
    \end{multicols}

    \newpage
    \section*{Übungsaufgaben und Musterlösungen}
\end{landscape}
\end{document}
